[["index.html", "Arbres sur données pharmacologiques Chapitre 1 1 - Introduction au projet 1.1 Mise en contexte 1.2 Notre cas, nos données 1.3 Etapes dun projet de data-science", " Arbres sur données pharmacologiques Valentin Constant 2021-11-16 Chapitre 1 1 - Introduction au projet 1.1 Mise en contexte Lintelligence artificielle a pour but de rendre une machine capable deffectuer des tâches a priori réservées aux êtres humains et aux animaux : percevoir, raisonner et agir. Il est possible de lappréhender de deux manières. Lapproche logique Lintelligence artificielle classique, telle que développée depuis les années 1950, se construit sur des principes logiques et des systèmes experts. Ces outils reproduisent le raisonnement dhumains experts dans un domaine. Pour comprendre son fonctionnement, prenez lexemple dun médecin qui pose un diagnostic. Lidée est : dobserver les modalités de ses prises de décision. Par exemple, SI jobserve ceci ET cela, ALORS je conclus que ; de reproduire ce cheminement de pensée dans un arbre décisionnel ; de coder ces règles avec des connecteurs logiques. Vous obtenez alors une machine capable de tirer toute seule des conclusions, à partir des informations que vous lui donnez. Ce système fonctionne plutôt bien, mais il suppose que la machine ait : une bonne mémoire pour stocker toutes les règles codées ; une grande puissance de calcul, car le programme est lourd à faire tourner. De plus, la collecte des données et leur codage se font à la main, ce qui représente un gros travail. Lapprentissage machine (machine learning) Ce courant estime que la logique ne reproduit quune facette de lintelligence humaine. Le cerveau possède en effet dautres méthodes, comme lanalogie, lintuition, etc. Comme le cerveau sentraîne et apprend en permanence, le courant neuronal tente de recréer cette capacité dapprentissage en sappuyant sur les neurosciences. Ces deux courants de recherche coexistent même si, selon les époques, lun a plus de succès que lautre. Un des buts de la recherche est de les articuler dans un courant unique. Aujourdhui, la plupart des applications de lintelligence artificielle sont un mélange des deux, adossé à de linformatique classique. Par exemple, une voiture autonome utilise : lapprentissage machine pour apprendre à détecter et analyser la route ; lapproche logique pour prendre des décisions de conduite. En résumé, lintelligence artificielle regroupe plusieurs courants et techniques pour aborder le fonctionnement de lintelligence. Si le courant logique cherche à reproduire les mécanismes du raisonnement humain, le courant neuronal veut reproduire la faculté dapprentissage. 1.2 Notre cas, nos données Lapproche logique de lintelligence sera privilégiée. En effet, nous utiliserons lIA pour raisonner et prendre des décisions. Nous disposons de données déjà prêtes. Elles proviennent détudes cliniques réalisées dans le laboratoire, sur la céfépime, la cefotaxime, la cefazoline, la pipéraciline et le méropénem. Nous disposons également dautres variables. DSTD : variable numérique, nombre décimal représentant combien de fois la dose standard a été administrée. continu : variable factorielle, informe du caractère continu de ladministration. INT_ : variable factorielle, informe sur la durée de lintervalle de dose en cas dadmnistration non continue. MOTTT_ : variable factorielle, informe sur le motif de traitement du patient. Fievre : variable factorielle, informe sur la présence de fièvre. SEXE : variable factorielle, informe sur le sexe du patient. PNAm : variable numérique, nombre décimal représentant lâge du patient en mois. taille : variable numérique, nombre décimal représentant la taille du patient en centimètre. BW : variable numérique, poids du patient en kilogrammes mesuré le jour de sa naissance. BWt : variable numérique, poids du patient lors de linclusion dans létude. PATHO_ : variable factorielle, informe sur la présence dune pathologie sur le patient. MOAD_ : variable factorielle, informe sur le motif dadmission du patient. EER : variable factorielle, informe sur lutilisation de lépuration extrarénale du patient. PELOD2 : variable numérique, score de gravité du patient. Dys_ : variable factorielle, informe sur la présence dun dysfonctionnement sur le patient. CRP : variable numérique, mesure de la protéine C réactive. PNN : variable numérique, mesure de la quantité de polynucléaire neutrophile. PQ : variable numérique, mesure de la quantité de plaquettes sanguines. ASAT : variable numérique, mesure de la quantité denzymes hépatiques (aspartate amino transférase) ALAT : variable numérique, mesure de la quantité denzymes hépatiques (alanine amino transférase) BiliC : variable numérique, mesure de la quantité de bilirubine conjuguée. UreeP : variable numérique, mesure de lurée plasmatique. CreatP : variable numérique, mesure de lurée plasmatique. Schwartz : variable numérique, critère de Schwartz (décrit le débit de filtration glomérulaire) CT : variable factorielle, utilisation dun cotraitement lors de ladministration. Diuretiques = variable factorielle, utilisation dun cotraitement lors de ladministration. Morphine = variable factorielle, utilisation dun cotraitement (morphine) lors de ladministration. BZD = variable factorielle, utilisation dun cotraitement (benzodiazépines) lors de ladministration. Curare = variable factorielle, utilisation dun cotraitement au Curare lors de ladministration. VasoP = variable factorielle, utilisation dun vasopresseurs lors de ladministration. Inotrope = variable factorielle, utilisation dun cotraitement lors de ladministration. Pres_germe = variable factorielle, informe si le germe a été identifié chez le patient. Pres_CMI = variable factorielle, informe si la CMI du germe a été identifiée chez le patient. enterobacteries = variable factorielle, informe si le germe identifié est de type entérobactérie. Haemophilus = variable factorielle, informe si le germe identifié est de type Haemophilus. kingella = variable factorielle, informe si le germe identifié est de type kingella. Moraxella = variable factorielle, informe si le germe identifié est de type Moraxella. Achromo = variable factorielle, informe si le germe identifié est de type Achromo. Neisseria = variable factorielle, informe si le germe identifié est de type Neisseria. Pyo = variable factorielle, informe si le germe identifié est de type Pyo. staph = variable factorielle, informe si le germe identifié est de type staphylocoque. Strepto = variable factorielle, informe si le germe identifié est de type Streptocoque enterocoq = variable factorielle, informe si le germe identifié est de type enterocoque. acineto = variable factorielle, informe si le germe identifié est de type acinetobacter. bulkho = variable factorielle, informe si le germe identifié est de type bulkho. Proba = variable factorielle, informe si le patient est sous-dosé. library(skimr) skimmed &lt;- skim(df) Ce qui fait un total de 597 dosages répartis sur 233 patients. 17 variables sont numériques et 78 sont factorielles. Sans être détaillé ici, ces données ont subis une phase dencodage (OneHot) et dimputation (constante pour les variables factorielles, aléatoire pour les variables numériques) 1.3 Etapes dun projet de data-science La première étape de Data Engineering consiste a explorer les données, les traiter pour les rendre exploitable, ce qui a été évoqué au paragraphe précédent. Ensuite, nous étudirons les prédicteurs, ici appelées features. Ce sont les variables permettant de prédire. Cette étape fait lobjet du chapitre suivant, intitulé Featuging Engineering, en français Ingénierie des prédicteurs, ou ingénierie des variables. Dans la partie Modélisation, nous avons 4 étapes majeures : la sélection du modèle, son entrainement, son évaluation et en dernier son optimisation. Cette étape fera lobjet du chapitre Modeling. "],["feature-engineering.html", "Chapitre 2 Feature engineering 2.1 La forêt aléatoire 2.2 Sélection, entrainement et tuning du modèle permettant le Feature Engineering 2.3 Importance des variables 2.4 Conclusion de la partie Feature Engineering", " Chapitre 2 Feature engineering 2.1 La forêt aléatoire Pour mesure limportance des variables, nous allons directement utiliser un modèle bien connu : la forêt aléatoire (Breiman, 2001). Petit rappel de ce quest une forêt aléatoire (différents arbres de décision entrainés sur des échantillons bootstrappés) La database va être divisée en deux : Le premier set, appelé set dentrainement, contiendra 75% des données. Le deuxième, appelé set de test, contiendra 25% des données. set.seed(123)#PSeudo aléatoire : fixer les seeds permet de reproduire le même hasard d&#39;une execution de code à une autre. smp_size &lt;- floor(0.75 * nrow(df)) #Seuil de 75% train_ind &lt;- sample(seq_len(nrow(df)), size = smp_size) #On mélange les lignes train_df &lt;- df[train_ind, ] #75% vont au train set test_df &lt;- df[-train_ind, ] #25% vont au test set 2.2 Sélection, entrainement et tuning du modèle permettant le Feature Engineering Pour réaliser ses 3 étapes dun coup, nous allons utiliser un outil très utilisé : Les grilles de recherche. Elles contiennent différentes valeurs dhyperparamètres. Chaque combinaison dhyperparamètre sera testé et la combinaison permettant la meilleure précision sera retenue pour le modèle. Rappel sur les principaux hyperparamètres dune forêt aléatoire : mtry : combien de variables candidates à la scission dun noeud ? splitrule : quelle règle régit la scission dun noeud ? min.node.size : taille minimale que doit contenir un noeud. rf_grid &lt;- expand.grid(mtry = seq(1:5), splitrule = c(&quot;gini&quot;, &quot;extratrees&quot;), min.node.size = c(1, 3, 5) ) group_fit_control &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 5) On entraine le modèle. model_rf &lt;- caret::train(Proba ~ ., data = train_df,method = &quot;ranger&quot;, #random forest trControl = group_fit_control, tuneGrid = rf_grid, importance=&quot;impurity&quot;) Résulat de lentrainement : model_rf$bestTune #Meilleur tuning ## mtry splitrule min.node.size ## 28 5 extratrees 1 Quelle est la précision de ce modèle sur un jeu de donnée inconnu ? On peut répondre à cette question grâce au jeu de test et à la matrice de confusion. test &lt;- test_df[which(names(test_df) != &quot;Proba&quot;)] predicted &lt;- predict(model_rf, test) caret::confusionMatrix(test_df$Proba, predicted) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 1 0 ## 1 68 9 ## 0 6 67 ## ## Accuracy : 0.9 ## 95% CI : (0.8404, 0.9429) ## No Information Rate : 0.5067 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.8001 ## ## Mcnemar&#39;s Test P-Value : 0.6056 ## ## Sensitivity : 0.9189 ## Specificity : 0.8816 ## Pos Pred Value : 0.8831 ## Neg Pred Value : 0.9178 ## Prevalence : 0.4933 ## Detection Rate : 0.4533 ## Detection Prevalence : 0.5133 ## Balanced Accuracy : 0.9002 ## ## &#39;Positive&#39; Class : 1 ## ## randomForest 4.6-14 ## Type rfNews() to see new features/changes/bug fixes. ## ## Attachement du package : &#39;randomForest&#39; ## L&#39;objet suivant est masqué depuis &#39;package:ggplot2&#39;: ## ## margin 2.3 Importance des variables Le modèle développé, nous allons pouvoir mesurer limportance des variables, avec la PFI (permutation feature importance) et lindice de GINI, lié aux forêts aléatoire. 2.3.1 PFI Petit rappel de ce quest la méthode de permutation (permutation aléatoire des valeurs des prédicteurs et mesure de limpact sur la prédiction) imp &lt;- FeatureImp$new(predictor, loss = &quot;ce&quot;, compare = &quot;difference&quot;, n.repetitions = 5) nom_var_perm &lt;- imp$results[1:21,1] val_var_perm &lt;- imp$results[1:21,3] barplot(val_var_perm, col=rainbow(25), main = &quot;Permutation Feature Importance&quot;, xlab = &quot;Feature&quot;, ylab = &quot;Importance&quot;) legend &lt;- nom_var_perm par(mar = c(0, 0, 0, 0)) plot.new() legend(&quot;top&quot;,legend,legend, col=rainbow(25), lty = 1, lwd = c(1, 1)) 2.3.2 Gini Les forêts aléatoires possèdent une méthode de mesure de limportance de variable intrinsèque : elle se base sur le critère de Gini. importance_gini &lt;- varImp(model_rf) imp1 &lt;- importance_gini[[&#39;importance&#39;]] #importance_gini$Overall &lt;- importance_gini$Overall / sum(importance_gini$Overall) (en %) impgini &lt;- data.frame(row.names(imp1),imp1$Overall) impgini_sorted &lt;- impgini[order(-impgini$imp1.Overall),] nom_var_gini &lt;- impgini_sorted$row.names.imp1.[1:20] val_var_gini &lt;- impgini_sorted$imp1.Overall[1:20] barplot(val_var_gini, col=rainbow(25), main = &quot;Mean decrease Gini importance&quot;, xlab = &quot;Feature&quot;, ylab = &quot;Importance&quot;) legend &lt;- nom_var_gini par(mar = c(0, 0, 0, 0)) plot.new() legend(&quot;top&quot;,legend,legend, lty = 1,col=rainbow(25), lwd = c(1, 1)) 2.3.3 RFE La RFE pousse le Feature Engineering un peu plus loin, au-delà dune simple mesure dimportance, il va aussi sélectionner un nombre de variable optimisé pour obtenir la précision la plus élevé possible. La RFE doit être tunnée : functions : quels modèles utiliser ? methods : quelle type de validation croisée? number : combien de folds lors de la validation croisée # ensure the results are repeatable set.seed(7) # load the library library(mlbench) library(caret) control_1 &lt;- rfeControl(functions=rfFuncs, method=&quot;cv&quot;, number=10) results &lt;- caret::rfe(df[,1:94],df$Proba, sizes=c(1:94), rfeControl=control_1) Résultats : nombre de variables retenu, nom de ces variables. results$optsize results$optVariables Graphique représentant lévolution de la précision du modèle selon le nombre de variable retenu. Le pic se trouve là où la précision est la plus élevée. Affichage des nom des variables retenues. plot(results, type=c(&quot;g&quot;, &quot;o&quot;)) nom_var_rfe &lt;- predictors(results)[1:20] legend &lt;- nom_var_rfe par(mar = c(0, 0, 0, 0)) plot.new() legend(&quot;top&quot;,legend,legend, lty = 1,col=rainbow(25), lwd = c(1, 1)) 2.4 Conclusion de la partie Feature Engineering Nous avons décidés de retenir les variables sélectionnées par la RFE. La PFI et Gini ont permis de confirmer les prédicteurs les plus importants. "],["modèle-à-13-variables.html", "Chapitre 3 Modèle à 13 variables 3.1 Vue des données 3.2 Entrainement, test et tuning du modèle 3.3 Matrice de confusion 3.4 Importance des variables 3.5 ALE - effets des variables 3.6 Un arbre au lieu dune forêt ? 3.7 Bilan", " Chapitre 3 Modèle à 13 variables ## ## Attachement du package : &#39;readr&#39; ## L&#39;objet suivant est masqué depuis &#39;package:scales&#39;: ## ## col_factor ## Les objets suivants sont masqués depuis &#39;package:vroom&#39;: ## ## as.col_spec, col_character, col_date, col_datetime, col_double, ## col_factor, col_guess, col_integer, col_logical, col_number, ## col_skip, col_time, cols, cols_condense, cols_only, date_names, ## date_names_lang, date_names_langs, default_locale, fwf_cols, ## fwf_empty, fwf_positions, fwf_widths, locale, output_column, ## problems, spec 3.1 Vue des données library(skimr) skimmed_ts &lt;- skim(df_third_selection) skimmed_ts Les prédicteurs catégoriels retenus sont : cefepime, informant sur lutilisation de la cefepime. piperaciline, informant sur lutilisation de la piperaciline. continu, informant sur le caractère continu de ladministration. INT_6H, informant sur le choix dun intervalle de dose de 6H en cas dadministration non continue. Pres_germe, informant sur lidentification du germe. enterobacteries, informant sur le type de germe (enterobacteries) en cas didentification. Pyo, informant sur le type de germe (Pyo) en cas didentification. Les prédicteurs quantitatifs sont : DSTD, informant sur la quantité de dose administrée (nombre de fois la dose standard) PNAm, âge du patient en mois. taille, taille du patient. BW, poids du patient à la naissance. BWt, poids du patient lors de linclusion dans létude. Schwartz, indice de Schwartz, reflet de la fonction rénale. 3.2 Entrainement, test et tuning du modèle smp_size &lt;- floor(0.75 * nrow(df_third_selection)) set.seed(123) train_ind_df_third_selection &lt;- sample(seq_len(nrow(df_third_selection)), size = smp_size) train_df_third_selection &lt;- df_third_selection[train_ind_df_third_selection, ] test_df_third_selection &lt;- df_third_selection[-train_ind_df_third_selection, ] rf_grid &lt;- expand.grid(mtry = seq(5:10), splitrule = c(&quot;gini&quot;, &quot;extratrees&quot;), min.node.size = c(1, 3, 5) ) group_fit_control &lt;- trainControl(method = &quot;cv&quot;, number = 10) model_rf_third_selec &lt;- caret::train(Proba ~ ., data = train_df_third_selection,method = &quot;ranger&quot;, #random forest trControl = group_fit_control, tuneGrid = rf_grid,importance=&quot;impurity&quot;) model_rf_third_selec$bestTune ## mtry splitrule min.node.size ## 23 4 extratrees 3 3.3 Matrice de confusion test_third_selection &lt;- test_df_third_selection[which(names(test_df_third_selection) != &quot;Proba&quot;)] predicted_third_selection &lt;- predict(model_rf_third_selec, test_third_selection) caret::confusionMatrix(test_df_third_selection$Proba, predicted_third_selection) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 1 0 ## 1 66 11 ## 0 5 68 ## ## Accuracy : 0.8933 ## 95% CI : (0.8326, 0.9378) ## No Information Rate : 0.5267 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.787 ## ## Mcnemar&#39;s Test P-Value : 0.2113 ## ## Sensitivity : 0.9296 ## Specificity : 0.8608 ## Pos Pred Value : 0.8571 ## Neg Pred Value : 0.9315 ## Prevalence : 0.4733 ## Detection Rate : 0.4400 ## Detection Prevalence : 0.5133 ## Balanced Accuracy : 0.8952 ## ## &#39;Positive&#39; Class : 1 ## 3.4 Importance des variables X_third_selection &lt;- df_third_selection[which(names(df_third_selection) != &quot;Proba&quot;)] predictor_third_selection &lt;- Predictor$new(model_rf_third_selec, data = X_third_selection, y = df_third_selection$Proba) 3.4.1 Méthode de permutation imp &lt;- FeatureImp$new(predictor_third_selection, loss = &quot;ce&quot;, compare = &quot;difference&quot;, n.repetitions = 5) plot(imp) importance_gini_opt_rf &lt;- varImp(model_rf_third_selec) plot(importance_gini_opt_rf) 3.4.2 LIME (Non compilé, utilisé pour déterminer les effets des variables sur une instance précise) model_randomforest &lt;- caret::train(Proba ~ ., data = train_df_third_selection,method = &quot;rf&quot;, #random forest trControl = group_fit_control) #LIME library(&quot;lime&quot;) explainer_2 &lt;- lime(train_df_third_selection, model_randomforest)#lime explanation_2 &lt;- lime::explain(test_df_third_selection[1, 1:13], explainer_2, n_labels = 1, n_features = 13) plot_explanations(explanation_2) plot_features(explanation_2) lime.rf &lt;- LocalModel$new(predictor_third_selection, k = 12, x.interest = X_third_selection[1,]) plot(lime.rf) shapley.rf &lt;- Shapley$new(predictor_third_selection, x.interest = X_third_selection[1,]) plot(shapley.rf) predict(model_randomforest, test_df_third_selection[1, ]) test_df_third_selection[1,14] 3.5 ALE - effets des variables (Non compilé, utilisé pour déterminer les effets des variables sur des intervalles de valeurs) ale_entero &lt;- FeatureEffect$new(predictor_third_selection, feature = &quot;enterobacteries&quot;) ale_entero$plot() ale_pyo &lt;- FeatureEffect$new(predictor_third_selection, feature = &quot;Pyo&quot;) ale_pyo$plot() ale_continu &lt;- FeatureEffect$new(predictor_third_selection, feature = &quot;continu&quot;) ale_continu$plot() ale_INT_6H &lt;- FeatureEffect$new(predictor_third_selection, feature = &quot;INT_6H&quot;) ale_INT_6H$plot() ale_cefe &lt;- FeatureEffect$new(predictor_third_selection, feature = &quot;cefe&quot;, grid.size = 5) ale_cefe$plot() ale_pipe &lt;- FeatureEffect$new(predictor_third_selection, feature = &quot;pipe&quot;, grid.size = 5) ale_pipe$plot() # FeatureEffect plots support up to two features: eff &lt;- FeatureEffect$new(predictor_third_selection, feature = c(&quot;taille&quot;, &quot;PNAm&quot;)) eff$plot(show.data = TRUE) eff &lt;- FeatureEffect$new(predictor_third_selection, feature = c(&quot;taille&quot;, &quot;BWt&quot;)) eff$plot(show.data = TRUE) eff &lt;- FeatureEffect$new(predictor_third_selection, feature = c(&quot;PNAm&quot;, &quot;BWt&quot;)) eff$plot(show.data = TRUE) eff &lt;- FeatureEffect$new(predictor_third_selection, feature = c(&quot;BWt&quot;, &quot;DSTD&quot;)) eff$plot(show.data = TRUE) eff &lt;- FeatureEffect$new(predictor_third_selection, feature = c(&quot;taille&quot;, &quot;DSTD&quot;)) eff$plot(show.data = TRUE) eff &lt;- FeatureEffect$new(predictor_third_selection, feature = c(&quot;PNAm&quot;, &quot;DSTD&quot;)) eff$plot(show.data = TRUE) 3.6 Un arbre au lieu dune forêt ? Historiquement, larbre de décision était utilisé bien avant les forêts aléatoires. Voyons ce que donnerait un arbre de décision sur nos données. tree_third &lt;- rpart(Proba ~ ., data=train_df_third_selection) prp(tree_third) tree_third ## n= 447 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 447 206 1 (0.53914989 0.46085011) ## 2) Pres_germe=0 199 40 1 (0.79899497 0.20100503) ## 4) DSTD&lt; 1.895 184 25 1 (0.86413043 0.13586957) * ## 5) DSTD&gt;=1.895 15 0 0 (0.00000000 1.00000000) * ## 3) Pres_germe=1 248 82 0 (0.33064516 0.66935484) ## 6) Pyo=1 33 2 1 (0.93939394 0.06060606) * ## 7) Pyo=0 215 51 0 (0.23720930 0.76279070) ## 14) BWt&gt;=44.5 12 2 1 (0.83333333 0.16666667) * ## 15) BWt&lt; 44.5 203 41 0 (0.20197044 0.79802956) ## 30) continu=0 151 41 0 (0.27152318 0.72847682) ## 60) Schwartz&gt;=126.034 83 33 0 (0.39759036 0.60240964) ## 120) cefe=0.0 45 21 1 (0.53333333 0.46666667) ## 240) enterobacteries=1 21 4 1 (0.80952381 0.19047619) * ## 241) enterobacteries=0 24 7 0 (0.29166667 0.70833333) * ## 121) cefe=1.0 38 9 0 (0.23684211 0.76315789) * ## 61) Schwartz&lt; 126.034 68 8 0 (0.11764706 0.88235294) * ## 31) continu=1 52 0 0 (0.00000000 1.00000000) * test_df_third_selection_x &lt;- test_df_third_selection[which(names(test_df_third_selection) != &quot;Proba&quot;)] predicted_third_selection &lt;- predict(tree_third, test_df_third_selection_x) predicted_third_selection &lt;- as.factor(ifelse(predicted_third_selection[,1] &lt; .5, 0, 1)) caret::confusionMatrix(reference = test_df_third_selection$Proba, data = predicted_third_selection, positive = &quot;1&quot;) ## Warning in confusionMatrix.default(reference = test_df_third_selection$Proba, : ## Levels are not in the same order for reference and data. Refactoring data to ## match. ## Confusion Matrix and Statistics ## ## Reference ## Prediction 1 0 ## 1 62 16 ## 0 15 57 ## ## Accuracy : 0.7933 ## 95% CI : (0.7197, 0.8551) ## No Information Rate : 0.5133 ## P-Value [Acc &gt; NIR] : 1.184e-12 ## ## Kappa : 0.5862 ## ## Mcnemar&#39;s Test P-Value : 1 ## ## Sensitivity : 0.8052 ## Specificity : 0.7808 ## Pos Pred Value : 0.7949 ## Neg Pred Value : 0.7917 ## Prevalence : 0.5133 ## Detection Rate : 0.4133 ## Detection Prevalence : 0.5200 ## Balanced Accuracy : 0.7930 ## ## &#39;Positive&#39; Class : 1 ## 3.7 Bilan La forêt entière, à 94 variables, a **91% de précision. La forêt optimisée, réduite à 13 variables, a 86,6%. Larbre seul, à 13 variable, a 79,3%. Ce projet a permis didentifié les variables les plus importantes pour optimiser le modèle, en vue de son utilisation. Pour utiliser ses modèles en ligne, il faut faire appel aux compétences dun data engineer. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
